import os
# Force clear proxies to avoid DashScope connection errors (Zombie Proxy issue)
os.environ.pop("HTTP_PROXY", None)
os.environ.pop("HTTPS_PROXY", None)
os.environ.pop("http_proxy", None)
os.environ.pop("https_proxy", None)

import cv2
import whisper
import dashscope
import json
import time
import base64
from http import HTTPStatus

# -----------------------------------------------------------------------------
# Asset A: ç§æœ‰æ³•è§„åº“
# -----------------------------------------------------------------------------
TRAINPAL_RULES = [
  { "id": "R001", "category": "ç«å“ä¸å“ç‰Œ", "severity": "CRITICAL", "description": "ç¦æ­¢å‡ºç°ç«å“Logoï¼ˆå¦‚LNER, Avantiï¼‰æˆ–ç”»é¢ã€‚", "triggers": ["LNER", "Avanti", "CrossCountry"] },
  { "id": "R002", "category": "ä»·æ ¼åˆè§„", "severity": "HIGH", "description": "ç¦æ­¢ä½¿ç”¨'æœ€ä¾¿å®œ'ã€'æœ€ä½ä»·'ç­‰ç»å¯¹åŒ–è¡¨è¿°ï¼Œé™¤éæœ‰è¯æ®ã€‚", "triggers": ["Cheapest", "Lowest Price", "Best", "æ— æ•Œ"] },
  { "id": "R003", "category": "æ”¿æ²»æ•æ„Ÿ", "severity": "CRITICAL", "description": "ç¦æ­¢å°†'é¦™æ¸¯'ä¸'ä¸­å›½'å¹¶åˆ—ï¼Œç¦æ­¢è¡¨è¿°'å»å¾€ä¸­å›½'ï¼Œå¿…é¡»ç¬¦åˆä¸€ä¸ªä¸­å›½åŸåˆ™ã€‚", "triggers": ["é¦™æ¸¯å»å¾€ä¸­å›½", "Hong Kong and China"] },
  { "id": "R004", "category": "ä¸æ–‡æ˜ç”»é¢", "severity": "HIGH", "description": "ç¦æ­¢å‡ºç°è¸¢ç®±å­ã€æŠ¢è€³æœºã€é“è·¯è„ä¹±å·®ç”»é¢ã€‚", "triggers": ["è¸¢ç®±å­", "æŠ¢è€³æœº", "è„ä¹±"] }
]

# -----------------------------------------------------------------------------
# Asset B: Qwen System Prompt
# -----------------------------------------------------------------------------
SYSTEM_PROMPT = """
ä½ æ˜¯ TrainPal å‡ºæµ·è¥é”€å›¢é˜Ÿçš„ AI å†…å®¹åˆè§„å®¡æ ¸åŠ©æ‰‹ã€‚ä½ å¿…é¡»ä»¥**é›¶å®¹å¿ (Zero Tolerance)** çš„æ€åº¦æ‰§è¡Œä»¥ä¸‹ 12 æ¡ç»å¯¹çº¢çº¿å®¡æ ¸æ ‡å‡†ã€‚

è¯·ç»“åˆ [Visual Analysis]ï¼ˆè§†è§‰ç”»é¢ï¼‰å’Œ [Audio Transcript]ï¼ˆéŸ³é¢‘æ–‡æœ¬ï¼‰ï¼Œé€å¸§é€å¥ä¸¥æ ¼æ’æŸ¥ã€‚

---
ğŸš¨ **12 æ¡ç»å¯¹çº¢çº¿ (Absolute Red Lines)**
---

**1ï¸âƒ£ ç«å“ä¸å“ç‰Œå…³ç³»**
âŒ è§†é¢‘ç”»é¢ä¸­å‡ºç°ä»»ä½•ç«è½¦è¿è¥å•† Logoï¼ˆLNER, Avanti, CrossCountry, GWR ç­‰ï¼‰â†’ **FAIL**
âŒ å‡ºç°ç«å“å“ç‰Œ Logo æˆ–ç”»é¢ç´ æ â†’ **FAIL**
âŒ è¸©ç«å“å“ç‰Œï¼ˆç”»é¢æ‰“å‰å…¶ä»–å“ç‰Œã€æ–‡æ¡ˆè¯´å…¶ä»–å“ç‰Œä¸å¥½ï¼‰â†’ **FAIL**
âœ… åªèƒ½å±•ç¤º TrainPal è‡ªå·±çš„äº§å“æˆªå›¾ï¼Œä¸”æˆªå›¾ä¸­ä¸èƒ½æœ‰è¿è¥å•† Logo

**2ï¸âƒ£ ä»·æ ¼ä¸ä¼˜æƒ åˆè§„**
âŒ ä½¿ç”¨ç»å¯¹åŒ–è¡¨è¿°ï¼š"æœ€ä¾¿å®œ"ã€"æœ€ä½ä»·æ ¼"ã€"æœ€ä½³"ã€"é¢†å…ˆ"ã€"æœ€æ£’"ã€"æ— æ•Œ" â†’ **FAIL**
âŒ è™šå‡ä»·æ ¼ P å›¾ã€è™šæ„åŸä»· â†’ **FAIL**
âŒ æœªåŠ é™å®šæ¡ä»¶çš„ä¼˜æƒ æ‰¿è¯ºï¼ˆå¦‚"ä¼¦æ•¦åˆ°æ›¼åŸ10é•‘ç›´è¾¾"ï¼‰â†’ **FAIL**
âœ… å¿…é¡»åŠ é™å®šæ¡ä»¶ï¼š"10é•‘èµ·ï¼ˆæå‰21å¤©é¢„è®¢/ä½å³°æ—¶æ®µï¼‰"

**3ï¸âƒ£ æ”¿æ²»ä¸æ•æ„Ÿå†…å®¹**
âŒ å°†"é¦™æ¸¯"ä¸"ä¸­å›½"å¹¶åˆ—æˆ–å¯¹ç«‹ï¼ˆå¦‚"é¦™æ¸¯å’Œä¸­å›½"ã€"Hong Kong and China"ï¼‰â†’ **FAIL**
âŒ ç§æ—/æ€§åˆ«/é˜¶çº§æ­§è§†å†…å®¹ â†’ **FAIL**
âŒ åäººè‚–åƒä¾µæƒï¼ˆåäºº memeã€è‚–åƒï¼‰â†’ **FAIL**
âŒ æ¶‰åŠé»„èµŒæ¯’ã€é…’ç²¾è¯ç‰©ã€å®—æ•™ã€æ”¿æ²»ã€è¡€è…¥æš´åŠ› â†’ **FAIL**

**4ï¸âƒ£ å†…å®¹çœŸå®æ€§**
âŒ å†·çŸ¥è¯†ä¸çœŸå®ã€æ— æ³•éªŒè¯ â†’ **FAIL**
âŒ è™šå‡è¯„è®º/å›å¤ç”¨æˆ· â†’ **FAIL**
âŒ è¯¯å¯¼ç”¨æˆ·æƒåˆ©ï¼ˆå¦‚"æ”¹ç­¾åˆ°17å¤©åå†é€€ç¥¨å°±èƒ½å…è´¹"æœªè¯´æ˜é™åˆ¶æ¡ä»¶ï¼‰â†’ **FAIL**

**5ï¸âƒ£ å¹¿å‘Šèº«ä»½æŠ«éœ²**
âŒ æ˜µç§°/ç®€ä»‹æåŠ TrainPal å“ç‰Œ â†’ **FAIL**
âŒ HashTag å¸¦ Tripã€Trainpal ç­‰è¯é¢˜ â†’ **FAIL**
âœ… è´¦å·å¿…é¡»äººè®¾åŒ–ï¼ˆå¦‚å¤§å­¦ç”Ÿã€çœé’±è¾¾äººã€é€€ä¼‘äººå‘˜ï¼‰

**6ï¸âƒ£ è‚–åƒæƒä¸éšç§ [æœ€é«˜ä¼˜å…ˆçº§]**
âŒ **æœªæ‰“ç è·¯äººæ­£è„¸/ä¾§è„¸/åŠè„¸** â†’ **FAIL (ç«‹å³åˆ¤å®š)**
   - åªè¦èƒ½çœ‹åˆ°çœ¼ç›ã€é¼»å­ã€å˜´å·´ç­‰é¢éƒ¨ç‰¹å¾ â†’ **FAIL**
   - å³ä½¿äººè„¸å¾ˆå°ã€åœ¨èƒŒæ™¯ä¸­ã€åªå‡ºç°ä¸€ç¬é—´ â†’ **ä¹Ÿå¿…é¡»æ ‡è®°è¿è§„**
âŒ ä½¿ç”¨çœŸäººç½‘ç»œç´ æå¤´åƒ â†’ **FAIL**
âŒ 5000ç²‰ä¸ä»¥ä¸Š KOL/ç½‘çº¢æ€¼è„¸è§†é¢‘ â†’ **FAIL**
âŒ åäººã€æ˜æ˜Ÿå‡ºé•œç”»é¢ â†’ **FAIL**
âŒ 10ä¸‡ä»¥ä¸Šç²‰ä¸ KOL æ­£è„¸å‡ºé•œ â†’ **FAIL**
âŒ å„¿ç«¥è‚–åƒï¼ˆå„åœ°åŒºå¯¹å„¿ç«¥è‚–åƒä¿æŠ¤ç‰¹åˆ«ä¸¥æ ¼ï¼‰â†’ **FAIL**

**7ï¸âƒ£ è¿æ³•ä¸æš´åŠ›å†…å®¹**
âŒ è¿æ³•çŠ¯ç½ªè¡Œä¸ºï¼ˆå¸æ¯’ã€èµŒåšã€é…—é…’ã€å·çªƒã€è¯ˆéª—ï¼‰â†’ **FAIL**
âŒ æš´åŠ›è¡€è…¥ï¼ˆå‡¶æ€ã€æ‰“æ¶æ–—æ®´ã€è‡ªæ®‹ã€è½¦ç¥¸ç°åœºã€æ‰‹æœ¯ç‰¹å†™ï¼‰â†’ **FAIL**
âŒ ææ€–ä¸»ä¹‰ï¼ˆæ¶‰åŠææ€–ç»„ç»‡ã€æç«¯ä¸»ä¹‰ã€é‚ªæ•™ï¼‰â†’ **FAIL**
âŒ é¥®é…’/æŠ½çƒŸç”»é¢è¥é€ "é¼“åŠ±ã€ç…½åŠ¨"æ°›å›´ â†’ **FAIL**

**8ï¸âƒ£ è‰²æƒ…ä½ä¿—**
âŒ æ˜ç¡®è‰²æƒ…ï¼ˆè£¸éœ²ç”Ÿæ®–å™¨å®˜ã€æ€§è¡Œä¸ºæˆ–æ¨¡ä»¿æ€§è¡Œä¸ºï¼‰â†’ **FAIL**
âŒ ç„¦ç‚¹æ€§æš—ç¤ºï¼ˆé•œå¤´é•¿æ—¶é—´èšç„¦èƒ¸éƒ¨ã€è‡€éƒ¨ã€å¤§è…¿æ ¹éƒ¨ï¼‰â†’ **FAIL**
âŒ æ€§æŒ‘é€—åŠ¨ä½œï¼ˆæ¨¡æ‹Ÿæ€§çˆ±åŠ¨ä½œã€åˆ»æ„æŠ–èƒ¸ã€æ’©è£™å­ï¼‰â†’ **FAIL**
âŒ è½¯è‰²æƒ…å†…å®¹ï¼ˆ"ç¦åˆ©å§¬"ã€æ€§æš—ç¤º ASMRã€å°ºåº¦æå¤§çš„èˆè¹ˆï¼‰â†’ **FAIL**

**9ï¸âƒ£ æœªæˆå¹´äººä¿æŠ¤**
âŒ ä»»ä½•æ¶‰åŠæœªæˆå¹´äººçš„æƒ…è‰²ã€è½¯è‰²æƒ…å†…å®¹ â†’ **FAIL**
âŒ æ¶æã€ç¾è¾±æœªæˆå¹´äººçš„ç”»é¢ï¼ˆå¦‚æ ¡å›­æš´åŠ›ç‰‡æ®µï¼‰â†’ **FAIL**
âŒ æ³„éœ²æœªæˆå¹´äººéšç§ï¼ˆå­¦æ ¡ã€å§“åã€å®¶åº­ä½å€ï¼‰â†’ **FAIL**

**ğŸ”Ÿ ç¦æ­¢æŠ¹é»‘é“è·¯æœåŠ¡è´¨é‡æˆ–ä»ä¸šäººå‘˜**
âŒ æš—ç¤ºé“è·¯ä¹˜åŠ¡å‘˜ã€å®¢æœ"æœ‰æ„éšç’"ã€"æ€åº¦å·®"ã€"ä¸ä½œä¸º" â†’ **FAIL**
âŒ ä½¿ç”¨"å†…éƒ¨ä»·"ã€"æ¼æ´"ã€"èµ°åé—¨" â†’ **FAIL**
âŒ æš—ç¤ºå®˜æ–¹åˆä½œ â†’ **FAIL**

**1ï¸âƒ£1ï¸âƒ£ ä¾›åº”å•†çº¢çº¿**
âŒ "åƒä¸‡ä¸è¦å»ç«è½¦ç«™è´­ç¥¨" â†’ **FAIL**
âŒ çº¿ä¸‹ä¹°ç¥¨è¯æœ¯ â†’ **FAIL**
âŒ äº‹æ•…/æ•…éšœç”»é¢ï¼ˆæœ‰æŸè‹±å›½é“è·¯å½¢è±¡çš„è§†é¢‘ç´ æï¼‰â†’ **FAIL**
âŒ ç«è½¦ä¾›åº”å•†è´Ÿé¢å¼•å¯¼å†…å®¹ â†’ **FAIL**

**1ï¸âƒ£2ï¸âƒ£ é£é™©ç”»é¢ç´ æ**
âŒ ç”·äººè¸¢ç®±å­ã€æŠ¢è€³æœºã€å¥³äººæ‰å‘é‡Œã€ç«è½¦ç€ç«ã€ä¹˜å®¢è·Ÿéšè¿›ç«™ç­‰ä¸æ–‡æ˜è¡Œä¸º â†’ **FAIL**

---
**åˆ¤å®šé€»è¾‘ (Strict Logic):**
- åªè¦å‘ç°ä¸Šè¿°ä»»ä½•ä¸€é¡¹è¿è§„ï¼ˆå“ªæ€•åªæœ‰ä¸€å¸§æˆ–ä¸€å¥è¯ï¼‰ï¼Œç»“æœå¿…é¡»æ˜¯ **FAIL**ã€‚
- **äººè„¸æ£€æµ‹æ˜¯æœ€é«˜ä¼˜å…ˆçº§**ï¼Œè¯·é€å¸§ä»”ç»†æ£€æŸ¥æ¯ä¸€å¼ ç”»é¢ã€‚
- åªæœ‰å½“è§†é¢‘å®Œå…¨å¹²å‡€ã€æ²¡æœ‰ä»»ä½•é£é™©ç‚¹æ—¶ï¼Œæ‰èƒ½ç»™ **PASS**ã€‚
- é£é™©è¯„åˆ† (Risk Score)ï¼šå‘ç°è¿è§„ç›´æ¥æ‰“ **100**ï¼Œæ— è¿è§„æ‰“ **0**ã€‚ä¸è¦ç»™ä¸­é—´åˆ†ã€‚

è¾“å‡ºå¿…é¡»ä¸ºçº¯ JSON æ ¼å¼ï¼ˆä¸è¦åŒ…å« ```json ... ```ï¼‰ï¼š
{
  "audit_result": "PASS" | "FAIL",
  "risk_score": 0 æˆ– 100,
  "violations": [{"timestamp": "xxs", "reason": "å‘ç°å¯è¯†åˆ«äººè„¸ï¼ˆçœ¼ç›+é¼»å­å¯è§ï¼‰/å‘ç°LNER Logo/ä½¿ç”¨ç»å¯¹åŒ–è¡¨è¿°'æœ€ä¾¿å®œ'/ç”·äººè¸¢ç®±å­ç”»é¢...", "category": "è‚–åƒéšç§/ç«å“ä¸å“ç‰Œ/ä»·æ ¼åˆè§„/é£é™©ç”»é¢ç´ æ..."}]
}
"""

class VideoAuditor:
    def __init__(self, api_key):
        self.api_key = api_key
        dashscope.api_key = api_key
        self.asr_model = None  # To hold the local Whisper model instance

    def _load_whisper(self):
        if self.asr_model is None:
            # Using 'base' model - will auto-download if network permits
            print("â³ Loading Whisper model (base)...")
            self.asr_model = whisper.load_model("base")

    def extract_audio(self, video_path):
        """
        Extracts audio/transcript using local Whisper (OpenAI).
        Running locally since ffmpeg is now installed!
        """
        try:
            self._load_whisper()
            result = self.asr_model.transcribe(video_path)
            # result['text'] contains the full transcript
            return result['text']
        except Exception as e:
            return f"[Whisper Error] {str(e)}"




    def extract_keyframes(self, video_path, interval_sec=None):
        """
        [è‡ªé€‚åº”å¸§é‡‡æ ·] Adaptive Frame Sampling via Scene Change Detection.
        - Detects significant visual changes (scene cuts).
        - Guaranteed max interval: 4s (to catch static violations).
        - Minimum interval: 1s (to avoid redundancy).
        """
        frames_paths = []
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            raise IOError(f"Cannot open video: {video_path}")
            
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps <= 0: fps = 25
        
        # Adaptive Parameters (Tuned for Face Detection)
        last_saved_time = -100
        min_interval = 0.8  # Min 0.8s between frames (faster capture)
        max_interval = 3.0  # Max 3s without frame (denser sampling)
        threshold = 18.0    # Lower threshold = more sensitive to subtle changes like faces
        
        prev_gray_frame = None
        
        # Ensure temp dir exists
        temp_dir = "temp_frames_qwen"
        os.makedirs(temp_dir, exist_ok=True)
        
        frame_idx = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            current_time = frame_idx / fps
            
            # Convert to gray for fast diffing
            small_frame = cv2.resize(frame, (320, 180)) # Resize for speed
            gray = cv2.cvtColor(small_frame, cv2.COLOR_BGR2GRAY)
            
            should_save = False
            time_since_last = current_time - last_saved_time
            
            # 1. First frame always save
            if last_saved_time < 0:
                should_save = True
                
            # 2. Max interval forced save
            elif time_since_last >= max_interval:
                should_save = True
                
            # 3. Scene Change Detection (only if min_interval passed)
            elif time_since_last >= min_interval:
                if prev_gray_frame is not None:
                    # Calculate difference score
                    score = cv2.mean(cv2.absdiff(gray, prev_gray_frame))[0]
                    if score > threshold:
                        should_save = True
            
            if should_save:
                # Save Full Resolution Frame (or slightly resized for API limit)
                # Resize to max 1024px width to save bandwidth/tokens
                h, w = frame.shape[:2]
                if w > 1024:
                    scale = 1024 / w
                    frame = cv2.resize(frame, (1024, int(h * scale)))
                
                # Burn Timestamp into Image (Visual Watermark for AI)
                # Text: "T: 4.2s"
                ts_text = f"T: {current_time:.1f}s"
                cv2.putText(frame, ts_text, (30, 80), 
                            cv2.FONT_HERSHEY_SIMPLEX, 2.5, (0, 0, 255), 5)

                frame_name = f"{temp_dir}/frame_{current_time:.2f}s.jpg"
                cv2.imwrite(frame_name, frame)
                frames_paths.append((round(current_time, 2), frame_name))
                
                last_saved_time = current_time
                prev_gray_frame = gray  # Update reference for next diff
            
            frame_idx += 1
            
        cap.release()
        print(f"ğŸ“¸ Adaptive Sampling: Extracted {len(frames_paths)} frames from video.")
        return frames_paths

    def audit(self, frames_data, audio_text, model_config=None):
        """
        Multimodal Audit: Supports Qwen (DashScope) and Generic OpenAI-Compatible (DeepSeek, GPT-4o, etc.)
        """
        # Default to Qwen if no config provided
        if model_config is None:
            model_config = {"type": "qwen", "model_name": "qwen-vl-max"}

        # Preparation: Instruction with Transcript
        if audio_text.startswith("[") and ("Error" in audio_text or "Exception" in audio_text):
            audio_instruction = f"\nã€Audio Transcriptã€‘:\n(éŸ³é¢‘æå–å¤±è´¥: {audio_text})\n\nâš ï¸ **ç‰¹åˆ«æŒ‡ä»¤**ï¼šä»…åŸºäºè§†è§‰ç”»é¢å®¡æ ¸ã€‚\n"
        else:
            audio_instruction = f"\nã€Audio Transcriptã€‘:\n\"{audio_text}\"\n\nè¯·ç»¼åˆåˆ†æç”»é¢ä¸éŸ³é¢‘ã€‚\n"

        prompt_final = f"{SYSTEM_PROMPT}\n{audio_instruction}\nå¿…é¡»è¿”å›çº¯ JSONã€‚"

        # --- BRANCH A: QWEN (DASHSCOPE SDK) ---
        if model_config['type'] == 'qwen':
            content = []
            for _, path in frames_data:
                with open(path, "rb") as f:
                    encoded = base64.b64encode(f.read()).decode('utf-8')
                    content.append({"image": f"data:image/jpeg;base64,{encoded}"})
            content.append({"text": prompt_final})
            
            try:
                response = dashscope.MultiModalConversation.call(
                    model='qwen-vl-max',
                    messages=[{"role": "user", "content": content}],
                    result_format='message'
                )
                if response.status_code == HTTPStatus.OK:
                    raw = response.output.choices[0].message.content
                    if isinstance(raw, list): raw = "".join([i['text'] for i in raw if 'text' in i])
                    return raw.replace("```json", "").replace("```", "").strip()
                return json.dumps({"audit_result": "ERROR", "error_msg": response.message})
            except Exception as e:
                return json.dumps({"audit_result": "ERROR", "error_msg": str(e)})

        # --- BRANCH B: OPENAI-COMPATIBLE (REQUESTS) ---
        else:
            import requests
            api_key = model_config.get('api_key')
            base_url = model_config.get('base_url', 'https://api.openai.com/v1').rstrip('/')
            model_name = model_config.get('model_name', 'gpt-4o')
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            # OpenAI Multimodal Content Format
            payload_content = [{"type": "text", "text": prompt_final}]
            
            for _, path in frames_data:
                # OPTIMIZATION: Resize & Compress to avoid 413 Entity Too Large
                # DeepSeek/OpenAI Gateway usually limits body size (e.g., 5MB-10MB).
                # Raw images will easily exceed this.
                try:
                    img = cv2.imread(path)
                    if img is not None:
                        # 1. Resize max dimension to 768px (Sufficient for audit)
                        h, w = img.shape[:2]
                        max_dim = max(h, w)
                        if max_dim > 768:
                            scale = 768 / max_dim
                            img = cv2.resize(img, (int(w*scale), int(h*scale)))
                        
                        # 2. Compress to JPEG with Quality 60
                        # This reduces size from ~500KB to ~30-50KB per frame
                        _, buffer = cv2.imencode('.jpg', img, [int(cv2.IMWRITE_JPEG_QUALITY), 60])
                        b64_img = base64.b64encode(buffer).decode('utf-8')
                        
                        payload_content.append({
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}
                        })
                except Exception as e:
                    print(f"Skipping frame {path} due to error: {e}")
            
            payload = {
                "model": model_name,
                "messages": [{"role": "user", "content": payload_content}],
                "max_tokens": 1024
            }
            
            try:
                resp = requests.post(f"{base_url}/chat/completions", headers=headers, json=payload, timeout=60)
                
                if resp.status_code == 200:
                    raw = resp.json()['choices'][0]['message']['content']
                    return raw.replace("```json", "").replace("```", "").strip()
                
                elif resp.status_code == 400 and ("image_url" in resp.text or "variant" in resp.text):
                     # Specific handling for models that don't support Vision (e.g., DeepSeek-V3 text-only)
                     raise Exception(f"Model does not support Vision inputs (400 Bad Request).")
                     
                else:
                    return json.dumps({"audit_result": "ERROR", "error_msg": f"API Error {resp.status_code}: {resp.text}"})
                    
            except Exception as e:
                # Engineering Best Practice: Failover Mechanism
                # If specialized model fails (network issue or capability issue), fallback to stable internal model
                print(f"âš ï¸ [Failover Triggered] Issue with {base_url}: {e}")
                print("ğŸ”„ Automatically switching to Reserve Model (Qwen-VL-Max)...")
                
                # Recursive call with Qwen config
                fallback_config = {"type": "qwen", "model_name": "qwen-vl-max (Fallback)"}
                return self.audit(frames_data, audio_text, model_config=fallback_config)


    @staticmethod
    def estimate_cost(token_usage):
        pass
