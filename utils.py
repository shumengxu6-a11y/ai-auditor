import os

import base64
import time
from moviepy import VideoFileClip
import google.generativeai as genai
from openai import OpenAI
from PIL import Image
import io

# ==========================================
# æ ¸å¿ƒä¸šåŠ¡è§„åˆ™ (Business Rules)
# ==========================================
# ==========================================
# æ ¸å¿ƒä¸šåŠ¡è§„åˆ™ (Business Rules)
# ==========================================
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„"TrainPal çŸ­è§†é¢‘åˆè§„å®¡æ ¸ä¸“å®¶"ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä»¥ä¸‹ã€ŠTrainPal åˆè§„çº¢çº¿ã€‹ä¸¥æ ¼å®¡æŸ¥è§†é¢‘çš„ç”»é¢ã€å­—å¹•åŠè¯­éŸ³å†…å®¹ã€‚

è¯·åŠ¡å¿…ä»ä»¥ä¸‹ **3 ä¸ªç‹¬ç«‹ç»´åº¦** è¿›è¡Œäº¤å‰å®¡æ ¸ï¼Œå¹¶æŒ‡å‡ºå…·ä½“çš„è¿è§„æ—¶é—´ç‚¹ï¼š

### 1. ğŸ‘ï¸ è¿è§„ç”»é¢ (Visual)
- **ç«å“æ’ä»–**: ä¸¥ç¦å‡ºç° "National Rail", "LNER", "Trainline" ç­‰é TrainPal æ ‡å¿—ã€‚
- **ä¸æ–‡æ˜è¡Œä¸º**: ä¸¥ç¦ "è„šè¸©åº§æ¤…", "æŠ¢å è€³æœº/æ‰‹æœº", "é†‰é…’/å¸çƒŸ"ã€‚
- **å®‰å…¨éšæ‚£**: ä¸¥ç¦å‡ºç°ç«è½¦ç€ç«ã€äº‹æ•…ã€ç”±äºå»¶è¯¯å¯¼è‡´çš„æ··ä¹±åœºé¢ã€‚

### 2. ğŸ“ è¿è§„å­—å¹• (Subtitle / OCR)
- **ä»·æ ¼åˆè§„**: ç¦æ­¢ç»å¯¹åŒ–æè¿°ï¼ˆå¦‚ "Cheapest", "No.1"ï¼‰ï¼›å¿…é¡»åŒ…å«æ¡ä»¶å‰ç¼€ï¼ˆ"From Â£10", "Up to 50% off"ï¼‰ã€‚
- **è™šå‡æ‰¿è¯º**: å¦‚ "Delay Repay" å¿…é¡»å¸¦ "Subject to T&Cs"ã€‚
- **æ”¿æ²»/æ•æ„Ÿè¯**: ä¸¥ç¦æ¶‰åŠé¦™æ¸¯/å°æ¹¾æ”¿æ²»é—®é¢˜åŠç§æ—æ­§è§†å†…å®¹ã€‚

### 3. ğŸ¤ è¿è§„é…éŸ³ (Audio / Dubbing)
- **è¯­éŸ³å†…å®¹**: å¿…é¡»å®¡æ ¸é…éŸ³å†…å®¹æ˜¯å¦åŒ…å«è¾±éª‚ã€è¯±å¯¼æ¶ˆè´¹æˆ–æ”¿æ²»æ•æ„Ÿè¯ã€‚
- **éŸ³ç”»ä¸€è‡´**: é…éŸ³æ‰¿è¯ºå¿…é¡»ä¸å­—å¹•æ¡æ¬¾ä¸€è‡´ï¼ˆä¾‹å¦‚é…éŸ³è¯´"å…¨é¢é€€æ¬¾"ä½†å­—å¹•å†™"éƒ¨åˆ†é€€æ¬¾"å³ä¸ºè¿è§„ï¼‰ã€‚

**è¾“å‡ºæ ¼å¼è¦æ±‚ (JSON)**ï¼š
è¯·ä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ï¼Œä¸è¦åŒ…å« Markdown ä»£ç å—æ ‡è®°ï¼š
{
  "is_compliant": true/false,
  "risk_score": 0-100,
  "issues": [
    {
      "timestamp": "MM:SS (ä¾‹å¦‚ 00:04)",
      "dimension": "ç”»é¢ / å­—å¹• / é…éŸ³",
      "category": "è¿è§„ç±»åˆ« (å¦‚: ç«å“æ’ä»–)",
      "description": "è¯¦ç»†æè¿°è¿è§„å†…å®¹ (ä¾‹å¦‚: ç”»é¢å·¦ä¸Šè§’å‡ºç° LNER å›¾æ ‡)",
      "suggestion": "å…·ä½“çš„ä¿®æ”¹å»ºè®® (ä¾‹å¦‚: ä½¿ç”¨é«˜æ–¯æ¨¡ç³Šé®ç›– LNER å›¾æ ‡)"
    }
  ]
}
å¦‚æœè§†é¢‘å®Œå…¨åˆè§„ï¼Œissues æ•°ç»„ä¸ºç©ºï¼Œrisk_score ä¸º 0ã€‚
"""

# ==========================================
# éŸ³é¢‘ä¸è§†é¢‘å¤„ç† (Audio & Video Processing)
# ==========================================

def get_video_duration(video_path):
    clip = VideoFileClip(video_path)
    dur = clip.duration
    clip.close()
    return dur

def extract_audio(video_path, output_path="temp_audio.mp3"):
    """
    ä»è§†é¢‘ä¸­æå–éŸ³é¢‘
    """
    try:
        clip = VideoFileClip(video_path)
        clip.audio.write_audiofile(output_path, logger=None)
        clip.close()
        return output_path
    except Exception as e:
        print(f"Audio extraction failed: {e}")
        return None

def extract_frames(video_path, fps=1, output_folder="temp_frames"):
    """
    ä»è§†é¢‘ä¸­æ¯ç§’æå– 1 å¸§ã€‚
    è¿”å›æå–çš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨ã€‚
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    # æ¸…ç†æ—§æ–‡ä»¶
    for f in os.listdir(output_folder):
        os.remove(os.path.join(output_folder, f))

    clip = VideoFileClip(video_path)
    duration = clip.duration
    frame_paths = []
    
    print(f"Video duration: {duration}s. Extracting 1 frame per second...")
    
    for t in range(0, int(duration) + 1):
        frame_path = os.path.join(output_folder, f"frame_{t}.jpg")
        clip.save_frame(frame_path, t=t)
        frame_paths.append(frame_path)
    
    clip.close()
    return frame_paths, duration

# ==========================================
# AI åˆ†æå¼•æ“ (Analysis Engine)
# ==========================================

def gemini_upload(video_path, api_key):
    """
    Step 1: Upload video to Gemini
    """
    genai.configure(api_key=api_key)
    print(f"Uploading {video_path} to Gemini...")
    return genai.upload_file(path=video_path)

def gemini_get_file(file_name):
    """
    Helper to get file status without blocking.
    """
    return genai.get_file(file_name)

def gemini_wait_for_processing(video_file, sleep_interval=2):
    """
    Step 2: Wait for video processing (blocks until ACTIVE)
    """
    while video_file.state.name == "PROCESSING":
        time.sleep(sleep_interval)
        video_file = genai.get_file(video_file.name)
    
    if video_file.state.name == "FAILED":
        raise ValueError(f"Video processing failed: {video_file.state.name}")
    
    return video_file

def gemini_generate_report(video_file, model_name="gemini-2.5-flash"):
    """
    Step 3: Analyze content
    """
    model = genai.GenerativeModel(model_name)
    response = model.generate_content(
        [SYSTEM_PROMPT, video_file],
        request_options={"timeout": 600}
    )
    return response.text

def analyze_video_gemini_native(video_path, api_key, model_name="gemini-2.5-flash"):
    """
    (Legacy Wrapper for backward compatibility if needed, but app.py should use new steps)
    """
    f = gemini_upload(video_path, api_key)
    f = gemini_wait_for_processing(f)
    return gemini_generate_report(f, model_name)

def transcribe_audio(audio_path, api_key, base_url):
    """
    ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ (Whisper) è¿›è¡Œè¯­éŸ³è½¬æ–‡å­—
    **Upgrade**: ä½¿ç”¨ verbose_json è·å–ç»†ç²’åº¦çš„æ—¶é—´æˆ³ (Segments)ï¼Œå®ç°ç²¾å‡†çš„éŸ³ç”»åŒæ­¥å®¡æ ¸ã€‚
    """
    client = OpenAI(api_key=api_key, base_url=base_url)
    audio_file = open(audio_path, "rb")
    try:
        # Request verbose_json to get segments and timestamps
        transcript = client.audio.transcriptions.create(
            model="whisper-1", 
            file=audio_file,
            response_format="verbose_json",
            timestamp_granularities=["segment"]
        )
        return transcript # Return the full object/dict
    except Exception as e:
        print(f"Transcription error: {e}")
        return None

def get_transcript_segment(transcript_obj, current_time):
    """
    æ ¹æ®å½“å‰è§†é¢‘æ—¶é—´æˆ³ï¼Œä» Whisper ç»“æœä¸­æå–å¯¹åº”çš„å­—å¹•ç‰‡æ®µã€‚
    """
    if not transcript_obj:
        return ""
    
    # Check if object or dict
    if hasattr(transcript_obj, 'segments'):
        segments = transcript_obj.segments
    elif isinstance(transcript_obj, dict) and 'segments' in transcript_obj:
        segments = transcript_obj['segments']
    else:
        return getattr(transcript_obj, 'text', str(transcript_obj)) # Fallback to full text

    context_text = []
    # æŸ¥æ‰¾è¦†ç›– current_time çš„ç‰‡æ®µï¼Œæˆ–è€…å‰å 2 ç§’çš„ç‰‡æ®µ (Context Window)
    for seg in segments:
        start = seg.get('start') if isinstance(seg, dict) else seg.start
        end = seg.get('end') if isinstance(seg, dict) else seg.end
        text = seg.get('text') if isinstance(seg, dict) else seg.text
        
        # å¦‚æœå½“å‰æ—¶é—´è½åœ¨ç‰‡æ®µèŒƒå›´å†…ï¼Œæˆ–è€…éå¸¸æ¥è¿‘
        if start <= current_time + 1 and end >= current_time - 1:
            context_text.append(f"[{start:.1f}s - {end:.1f}s]: {text}")
            
    return "\n".join(context_text)

def analyze_frame_gemini(image_path, api_key, model_name="gemini-2.5-flash"):
    # (ä¿ç•™ç”¨äº fallback)
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(model_name)
    img = Image.open(image_path)
    try:
        response = model.generate_content([SYSTEM_PROMPT, img])
        return response.text
    except Exception as e:
        return f"Error: {str(e)}"

def analyze_frame_openai_compatible(image_path, api_key, base_url, model_name="gpt-4o", audio_context=None):
    """
    ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ (å¦‚ DeepSeek, Moonshot ç­‰)
    æ–°å¢ audio_context ä»¥æ”¯æŒè¯­éŸ³è½¬å†™å†…å®¹çš„è¯­ä¹‰å®¡æ ¸ã€‚
    """
    client = OpenAI(api_key=api_key, base_url=base_url)
    
    # Encode image
    with open(image_path, "rb") as image_file:
        base64_image = base64.b64encode(image_file.read()).decode('utf-8')

    # Build prompt
    user_content_list = [
        {"type": "text", "text": "è¯·å®¡æ ¸è¿™å¼ è§†é¢‘æˆªå›¾æ˜¯å¦åˆè§„ã€‚"}
    ]
    
    if audio_context:
        user_content_list.append({
            "type": "text",
            "text": f"\n\nã€é™„åŠ å¤šæ¨¡æ€ä¿¡æ¯ (Audio Transcript)ã€‘\nå½“å‰è§†é¢‘ç‰‡æ®µçš„è¯­éŸ³è½¬æ–‡å­—å†…å®¹å¦‚ä¸‹ï¼Œè¯·ç»“åˆè¿™äº›å†…å®¹è¾…åŠ©å®¡æ ¸é…éŸ³è¿è§„ä¸éŸ³ç”»ä¸€è‡´æ€§ï¼š\n{audio_context}"
        })

    user_content_list.append({
        "type": "image_url",
        "image_url": {
            "url": f"data:image/jpeg;base64,{base64_image}"
        }
    })

    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "system",
                    "content": SYSTEM_PROMPT
                },
                {
                    "role": "user",
                    "content": user_content_list
                }
            ],
            max_tokens=2000 
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error: {str(e)}"

def estimate_cost(duration_seconds, model_name="gemini-2.5-flash"):
    """
    ä¼°ç®—æˆæœ¬ (Based on 2025 Public Pricing - Estimates only)
    Ref: https://ai.google.dev/pricing
    Ref: https://openai.com/pricing
    """
    # Token Estimation
    # Video: Gemini charges ~263 tokens/second. GPT-4o Vision: ~85-170 tokens/frame (low res) or 1000+ (high res).
    # Audio: Whisper ~ $0.006 / min.
    
    total_cost = 0.0
    details = ""
    
    # 1. Google Gemini Pricing (Flash is extremely cheap)
    if "gemini" in model_name.lower():
        if "flash" in model_name.lower():
            price_per_1m_tokens = 0.10 # $0.10 input / 1M tokens
        else: # Pro
            price_per_1m_tokens = 2.50 # $2.50 input / 1M tokens (approx)
            
        estimated_tokens = duration_seconds * 300 # Video + Audio tokens
        total_cost = (estimated_tokens / 1_000_000) * price_per_1m_tokens
        details = f"Gemini ({model_name}): {estimated_tokens} tokens @ ${price_per_1m_tokens}/1M"

    # 2. OpenAI / DeepSeek Pricing
    else:
        # GPT-4o / DeepSeek (Vision + Whisper)
        frames = duration_seconds # 1 FPS
        
        # Whisper Cost
        whisper_cost = (duration_seconds / 60) * 0.006
        
        # Vision Cost (Assumption: Low-res detail for cost efficiency or High-res for quality)
        # DeepSeek is cheaper, GPT-4o is expensive.
        if "deepseek" in model_name.lower():
            # DeepSeek V3/R1 is approx $0.14-$0.55 / 1M tokens. 
            # Vision capabilities via API vary, assuming similar token mapping.
            vision_price_per_image = 0.0002 
        elif "mini" in model_name.lower():
            vision_price_per_image = 0.0005 # GPT-4o-mini
        else:
            vision_price_per_image = 0.005 # GPT-4o Standard (expensive)
            
        vision_cost = frames * vision_price_per_image
        total_cost = whisper_cost + vision_cost
        details = f"{model_name}: Whisper(${whisper_cost:.4f}) + Vision({frames}f * ${vision_price_per_image})"
        
    return total_cost, details 
